{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3rGTK43YrNz"
      },
      "source": [
        "# MIE524 - Assignment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5Fl4bBY4k5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVBFUi_sY_Rl"
      },
      "source": [
        "Let's set up Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV-AdHT2Yk_4",
        "outputId": "e54dd0a1-f0f7-410d-fc8a-eae81fcd01d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u382-ga-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCNkNzbNZEOp"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "LFZXyGgJZISe"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3flXijoNxGW5"
      },
      "source": [
        "Put all your imports, and path constants in the next cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "UZlFnHCuxPik"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBzZCFTUZVIO"
      },
      "source": [
        "## Q1 - Word Count in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APxI9MCs4Ovz"
      },
      "source": [
        "### Write your function in the next cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "EKLLX-PVzekR"
      },
      "outputs": [],
      "source": [
        "# load the text\n",
        "rdd = spark.sparkContext.textFile(\"/content/plotsummaries.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "82TmJ8Ii3u4q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def lower_str(word):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        word: string\n",
        "    OUTPUT:\n",
        "        modified_wrod: string\n",
        "\n",
        "    NOTE: output the given word in lower case letters.\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    res = word.lower()\n",
        "    return (res)\n",
        "\n",
        "def strip_punc(word):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        word: string\n",
        "    OUTPUT:\n",
        "        modified_wrod: string\n",
        "\n",
        "    NOTE: output the given word with characters stripped.\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    punc = '!\"#$%&\\'()*+,.:;<=>?@[\\]^_`{|}-~/'\n",
        "\n",
        "    for p in punc:\n",
        "      word = word.replace(p, '')\n",
        "    return (word)\n",
        "\n",
        "# You may have additional functions\n",
        "\n",
        "def remove_num(word):\n",
        "  if any(char.isnumeric() for char in word): # isnumeric vs. isdigit - isnumeric cover bigger scope so was selected\n",
        "    return True\n",
        "  return(False)\n",
        "\n",
        "# def remove_nonEng(word):\n",
        "#   if any(char.isalpha() for char in word):\n",
        "#     return True\n",
        "#   return(False)\n",
        "\n",
        "def remove_nonEnglish(word):\n",
        "  # initially tried to use .isAlpha() but wasn't fully doing the work\n",
        "  english_pattern = re.compile(r'^[a-zA-Z]+$') # alphabet\n",
        "  return(english_pattern.match(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcXHqu7q6IwZ"
      },
      "source": [
        "Run your function in the next cells to output required content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAIIrExX5_H-",
        "outputId": "d07e204e-6c6e-4ac8-b5e3-314fc68b20e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 183688),\n",
              " ('aa', 21),\n",
              " ('aaa', 1),\n",
              " ('aachan', 2),\n",
              " ('aachen', 1),\n",
              " ('aachim', 8),\n",
              " ('aaden', 1),\n",
              " ('aag', 7),\n",
              " ('aaianna', 4),\n",
              " ('aalim', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "# filtering process\n",
        "rdd = rdd.map(lower_str).map(strip_punc) # to lowercase & remove punctuations\n",
        "rdd = rdd.flatMap(lambda line: line.split(\" \")) # to word\n",
        "rdd = rdd.filter(lambda word: not remove_num(word)) # remove number\n",
        "rdd = rdd.filter(lambda word: remove_nonEnglish(word))\n",
        "\n",
        "rdd = rdd.filter(lambda x:x!='') # remove ''\n",
        "\n",
        "# reduceByKey(): count how many times each word occurs\n",
        "rdd_num_init = rdd.map(lambda word:(word,1)) # initialization, there will be repetition\n",
        "rdd_num = rdd_num_init.reduceByKey(lambda x,y:(x+y)).sortByKey() # condense\n",
        "\n",
        "## OUTPUT\n",
        "rdd_num.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "QxkE7L91A_ND"
      },
      "outputs": [],
      "source": [
        "# mask to only leave the first character\n",
        "masked_rdd = rdd_num.map(lambda word: (word[0][0], word[1]))\n",
        "\n",
        "# combine the values that share key\n",
        "res_rdd = masked_rdd.reduceByKey(lambda x, y: (x + y)).sortByKey()\n",
        "\n",
        "res = res_rdd.collect()\n",
        "\n",
        "with open('Q1.txt', 'w') as saveFile:\n",
        "  for r in res:\n",
        "    saveFile.write(str(r).strip(\"()\"))\n",
        "    saveFile.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYACdWmz6MUT"
      },
      "source": [
        "## PART 2 - Oxford Covid-19 Government Response Tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cvzdbj37zyh"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "e-1cILHlIEVJ"
      },
      "outputs": [],
      "source": [
        "## COMBINE LATER... but as we don't want to load everytime\n",
        "\n",
        "# load the dataset\n",
        "rdd = spark.read.option(\"header\", True).csv(\"/content/OxCGRT_USA_latest.csv\")\n",
        "# print(rdd.columns)\n",
        "# rdd.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFUIJwlD8KiM"
      },
      "source": [
        "### Q2 - Computing Index Score with Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "QR8Y5qs4eEtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee2d03b-28ab-482f-80b6-ce970f0b69b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['C1_Flag', 'C2_Flag', 'C3_Flag', 'C4_Flag', 'C5_Flag', 'C6_Flag', 'C7_Flag', 'E1_Flag', 'H1_Flag', 'H6_Flag', 'H7_Flag', 'H8_Flag']\n",
            "dict_keys(['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'E1', 'H1', 'H6', 'H7', 'H8'])\n"
          ]
        }
      ],
      "source": [
        "from decimal import InvalidOperation\n",
        "# Treat as Global Variable\n",
        "\n",
        "indicators = [\"C1M_School closing\",\n",
        "    \"C2M_Workplace closing\",\n",
        "    \"C3M_Cancel public events\",\n",
        "    \"C4M_Restrictions on gatherings\",\n",
        "    \"C5M_Close public transport\",\n",
        "    \"C6M_Stay at home requirements\",\n",
        "    \"C7M_Restrictions on internal movement\",\n",
        "    \"C8EV_International travel controls\",\n",
        "    \"E1_Income support\",\n",
        "    \"E2_Debt/contract relief\",\n",
        "    \"H1_Public information campaigns\",\n",
        "    \"H2_Testing policy\",\n",
        "    \"H3_Contact tracing\",\n",
        "    \"H6M_Facial Coverings\",\n",
        "    \"H7_Vaccination policy\",\n",
        "    \"H8M_Protection of elderly people\"]\n",
        "\n",
        "indicator_flags = [\"C1M_Flag\",\n",
        "    \"C2M_Flag\",\n",
        "    \"C3M_Flag\",\n",
        "    \"C4M_Flag\",\n",
        "    \"C5M_Flag\",\n",
        "    \"C6M_Flag\",\n",
        "    \"C7M_Flag\",\n",
        "    \"E1_Flag\",\n",
        "    \"H1_Flag\",\n",
        "    \"H6M_Flag\",\n",
        "    \"H7_Flag\",\n",
        "    \"H8M_Flag\"]\n",
        "\n",
        "indicator_lst = [column[:2] for column in indicators]\n",
        "indicator_flags_filtered = [flag.replace('M', '') for flag in indicator_flags] # for convinence\n",
        "FLAG = {flag[:2]: 1 if any(flag[:2] in indic_flags[:2] for indic_flags in indicator_flags) else 0 for flag in indicator_flags_filtered} # C8, H2, H3, H8, E2: flag values dont exist\n",
        "print(indicator_flags_filtered)\n",
        "\n",
        "max_lst = [3, 3, 2, 4, 2, 3, 2, 4, 2, 2, 2, 3, 2, 4, 5, 3]\n",
        "MAX = {indicator: max_lst[indicator_lst.index(indicator)] for indicator in indicator_lst}\n",
        "\n",
        "needs = [\"RegionName\", \"Date\"]\n",
        "\n",
        "print(FLAG.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "G4VuOGh78O4r"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from functools import reduce\n",
        "from collections import Counter\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        df: spark dataframe\n",
        "    OUTPUT:\n",
        "        cleaned data: spark dataframe\n",
        "\n",
        "    NOTE: output the given word with characters stripped.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # extract only the columns to use\n",
        "    extract = needs + indicators + indicator_flags\n",
        "    extract_df = df.select(*extract)\n",
        "\n",
        "    # drop the rows that have RegionName empty (NA)\n",
        "    clean_df = extract_df.na.drop(subset=['RegionName'])\n",
        "\n",
        "    # cast the value to integer\n",
        "    for col_name in clean_df.columns:\n",
        "      if col_name not in needs:\n",
        "        clean_df = clean_df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
        "\n",
        "    # fill up the empty values\n",
        "    clean_df = clean_df.na.fill(0) # fill with minimum value: 0 (indicator and flag)\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "def personal_mode(nums):\n",
        "  counts = Counter(nums)\n",
        "  max_count = max(counts.values())\n",
        "  mode = [item for item, count in counts.items() if count == max_count] # list of all that have max_count\n",
        "\n",
        "  if len(mode) > 1: # if multiple modes, take the largest value\n",
        "      mode = max(mode)\n",
        "  else:\n",
        "      mode = mode[0]  # if only one, take that\n",
        "  return mode\n",
        "\n",
        "personal_mode = udf(personal_mode, IntegerType())\n",
        "\n",
        "\n",
        "\n",
        "def impute_data(df):\n",
        "  # min value to fill NA values -> take \"mode\" to aggregate by month -> if there is tie in \"mode\" then break it by taking the bigger one\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        df: spark dataframe\n",
        "    OUTPUT:\n",
        "        imputed data: spark dataframe\n",
        "\n",
        "    NOTE: output the dataframe with nan values replaced with the minimal value ofT the given indicator.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    clean_indic_df = df.select([col(old).alias(old[:2]) if old in indicators else old for old in df.columns]) # manipulate the header to only leave 'indicator'\n",
        "    formated_df = clean_indic_df.withColumn(\"Date\", date_format(to_date(col(\"Date\"), \"yyyyMMdd\"), \"yyyyMM\")) # format manipulation; to date\n",
        "\n",
        "    # with default mode ; couldnt find a resource of how it handles tie break\n",
        "    df_imputed1 = formated_df.groupby(\"RegionName\", \"Date\").agg(*[mode(indicator).alias(indicator) for indicator in indicator_lst])\n",
        "    df_imputed2 = formated_df.groupby(\"RegionName\", \"Date\").agg(*[mode(indicator_flag).alias(indicator_flag.replace('M','')) for indicator_flag in indicator_flags])\n",
        "\n",
        "    # Attempted with TIE BREAKER mode - wasn't able to implement\n",
        "    # df_imputed1 = formated_df.groupBy(\"RegionName\", \"Date\", *indicator_lst).agg(*[personal_mode((col(indicator)).alias(indicator)) for indicator in indicator_lst])\n",
        "    # df_imputed2 = formated_df.groupBy(\"RegionName\", \"Date\", *indicator_flags).agg(*[personal_mode((col(indicator_flag)).alias(indicator_flag)) for indicator_flag in indicator_flags])\n",
        "\n",
        "    df_imputed = df_imputed1.join(df_imputed2, [\"RegionName\", \"Date\"], \"left\")\n",
        "    return (df_imputed)\n",
        "\n",
        "\n",
        "def create_I(df):\n",
        "  # compute I value for each indicator and save to lst\n",
        "  lst = []\n",
        "  for ind in indicator_lst:\n",
        "    v = df[ind]\n",
        "    n = MAX[ind]\n",
        "    if v == 0: # if v = 0 -> I = 0\n",
        "      I = 0\n",
        "\n",
        "    elif ind not in FLAG.keys(): # if F = 0 -> f = 0\n",
        "      F = 0\n",
        "      f = 0\n",
        "      I = 100 * (v - 0.5*(F - f)) / n\n",
        "\n",
        "    else:\n",
        "      F = FLAG[ind]\n",
        "      f = df[ind+\"_Flag\"]\n",
        "      I = 100 * (v - 0.5*(F - f)) / n\n",
        "\n",
        "    lst.append(I)\n",
        "\n",
        "  return(lst)\n",
        "\n",
        "\n",
        "def compute_index_score(df):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        df: spark dataframe\n",
        "    OUTPUT:\n",
        "        list of index scores per region and period: list\n",
        "\n",
        "    NOTE: output a list of computed scores per region and period based on the algorithm.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # traverse over row and apply the function\n",
        "    res = df.rdd.map(lambda x: create_I(x))\n",
        "\n",
        "    # calculate the sum of res\n",
        "    row_sums = res.map(lambda row: reduce(lambda x, y: x + y, row, 0.0)).collect()\n",
        "\n",
        "    # calculate Government Index\n",
        "    Gov_Index = [1/16*val for val in row_sums]\n",
        "\n",
        "    # dataframe of government index\n",
        "    gov_index_df = spark.createDataFrame([(gov_index,) for gov_index in Gov_Index], [\"Gov_Index\"])\n",
        "    # dataframe of RegionName and Date\n",
        "    selected_df = df.select('RegionName', 'Date').orderBy(\"RegionName\", \"Date\")\n",
        "\n",
        "\n",
        "    # add a unique identifier to join them\n",
        "    gov_index_df = gov_index_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "    selected_df = selected_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "    # join by id and drop the extra column\n",
        "    combined_df = selected_df.join(gov_index_df, \"id\", \"inner\").drop(\"id\")\n",
        "\n",
        "    # reorder the columns as expected output\n",
        "    combined_df = combined_df.select(\"RegionName\", \"Date\", \"Gov_Index\")\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEocWs4stsKs"
      },
      "source": [
        "Run your function in the next cells to output required content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "OReazs5stsbh"
      },
      "outputs": [],
      "source": [
        "let = clean_data(rdd)\n",
        "# let.printSchema()\n",
        "# let.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "INif5zU3bpZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e48339f-32ae-4984-9220-cf9e1fdb238e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|RegionName|  Date| C1| C2| C3| C4| C5| C6| C7| C8| E1| E2| H1| H2| H3| H6| H7| H8|C1_Flag|C2_Flag|C3_Flag|C4_Flag|C5_Flag|C6_Flag|C7_Flag|E1_Flag|H1_Flag|H6_Flag|H7_Flag|H8_Flag|\n",
            "+----------+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|   Alabama|202001|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|\n",
            "|   Alabama|202002|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  1|  1|  0|  0|  0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|\n",
            "|   Alabama|202003|  0|  0|  2|  0|  0|  0|  1|  3|  0|  0|  2|  1|  1|  0|  0|  3|      1|      0|      0|      0|      0|      0|      1|      0|      0|      0|      0|      1|\n",
            "|   Alabama|202004|  3|  3|  2|  4|  1|  2|  1|  3|  2|  1|  2|  1|  1|  1|  0|  3|      1|      1|      1|      1|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202005|  3|  3|  2|  0|  1|  2|  1|  3|  2|  1|  2|  2|  1|  3|  0|  3|      0|      0|      1|      0|      0|      0|      1|      0|      1|      0|      0|      1|\n",
            "|   Alabama|202006|  3|  3|  1|  0|  1|  2|  1|  3|  2|  1|  2|  3|  1|  3|  0|  3|      0|      0|      1|      0|      0|      0|      1|      0|      1|      0|      0|      1|\n",
            "|   Alabama|202007|  3|  1|  1|  0|  1|  1|  1|  3|  2|  1|  2|  3|  1|  3|  0|  3|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202008|  2|  1|  1|  0|  0|  1|  1|  3|  1|  0|  2|  3|  1|  3|  0|  3|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202009|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  3|  2|  3|  0|  3|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202010|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  3|  1|  3|  0|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202011|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  3|  1|  3|  0|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      0|      1|\n",
            "|   Alabama|202012|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  3|  1|  3|  0|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202101|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  3|  1|  3|  2|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202102|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  2|  1|  3|  2|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202103|  2|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  2|  1|  3|  2|  2|      0|      1|      1|      0|      0|      1|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202104|  1|  1|  1|  0|  0|  1|  1|  3|  1|  1|  2|  2|  1|  2|  5|  2|      1|      1|      1|      0|      0|      1|      1|      0|      1|      0|      1|      1|\n",
            "|   Alabama|202105|  1|  0|  1|  0|  0|  1|  1|  3|  1|  1|  2|  2|  1|  2|  5|  2|      1|      0|      1|      0|      0|      1|      1|      0|      1|      0|      1|      1|\n",
            "|   Alabama|202106|  0|  0|  1|  0|  0|  1|  1|  3|  1|  1|  2|  2|  1|  2|  5|  1|      0|      0|      1|      0|      0|      1|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202107|  0|  0|  1|  0|  0|  0|  1|  3|  1|  1|  2|  2|  1|  2|  5|  1|      0|      0|      1|      0|      0|      0|      1|      0|      1|      1|      1|      1|\n",
            "|   Alabama|202108|  1|  0|  0|  0|  0|  0|  1|  3|  1|  1|  2|  3|  1|  2|  5|  1|      1|      0|      0|      0|      0|      0|      1|      0|      1|      0|      1|      1|\n",
            "+----------+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let2 = impute_data(let)\n",
        "let2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "sCUWRbsYiB6_"
      },
      "outputs": [],
      "source": [
        "let3 = compute_index_score(let2)\n",
        "\n",
        "let3 = let3.collect()\n",
        "with open('Q2.txt', 'w') as saveFile:\n",
        "    for row in let3:\n",
        "        # Extract values from the row\n",
        "        region_name = row[\"RegionName\"]\n",
        "        date = row[\"Date\"]\n",
        "        gov_index = row[\"Gov_Index\"]\n",
        "\n",
        "        # Format and write the values to the file\n",
        "        saveFile.write(f'{region_name}, {date}, {gov_index}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d6NcDhRtuTK"
      },
      "source": [
        "### Q3 - Association Rules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset - if didn't above\n",
        "rdd = spark.read.option(\"header\", True).csv(\"/content/OxCGRT_USA_latest.csv\")"
      ],
      "metadata": {
        "id": "cbAkdZ_P-m-V"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "iJT7Xg0R9K6C"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "def get_data(df): # MODIFIED THE FUNCTION FROM Q2\n",
        "    # extract only the columns in use\n",
        "    extract = needs + indicators\n",
        "    extract_df = df.select(*extract)\n",
        "\n",
        "    # drop the rows that have RegionName empty (NA)\n",
        "    clean_df = extract_df.na.drop(subset=['RegionName'])\n",
        "\n",
        "    # cast the value to integer\n",
        "    for col_name in clean_df.columns:\n",
        "      if col_name not in needs:\n",
        "        clean_df = clean_df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
        "\n",
        "    # cast datatype of date\n",
        "    clean_df = clean_df.withColumn('Date', to_date(col('Date'), 'yyyyMMdd'))\n",
        "\n",
        "    # fill up the None / Null values\n",
        "    clean_df = clean_df.na.fill(0) # fill with minimum value: 0 (indicator and flag)\n",
        "\n",
        "    # extract out the year and month, used to do partitioning to extract first day of each month\n",
        "    clean_df = clean_df.withColumn('year', year(col('Date')))\n",
        "    clean_df = clean_df.withColumn('month', month(col('Date')))\n",
        "\n",
        "    # spec for partitioning and ordering\n",
        "    spec = Window.partitionBy(\"RegionName\", \"year\", \"month\").orderBy(\"Date\")\n",
        "\n",
        "    # add a row number to each row within the partition\n",
        "    clean_df = clean_df.withColumn(\"row_num\", row_number().over(spec))\n",
        "\n",
        "    # filter and get the first row in each partition\n",
        "    clean_df = clean_df.filter(clean_df.row_num == 1)\n",
        "\n",
        "    # drop unnecesary columns\n",
        "    clean_df = clean_df.drop(\"row_num\", \"year\", \"month\")\n",
        "\n",
        "    return clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Tv-C-S3f9RCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d14d44b-2e05-4d25-b222-771b2b3c6a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------------------+---------------------+------------------------+------------------------------+--------------------------+-----------------------------+-------------------------------------+----------------------------------+-----------------+-----------------------+-------------------------------+-----------------+------------------+--------------------+---------------------+--------------------------------+\n",
            "|RegionName|      Date|C1M_School closing|C2M_Workplace closing|C3M_Cancel public events|C4M_Restrictions on gatherings|C5M_Close public transport|C6M_Stay at home requirements|C7M_Restrictions on internal movement|C8EV_International travel controls|E1_Income support|E2_Debt/contract relief|H1_Public information campaigns|H2_Testing policy|H3_Contact tracing|H6M_Facial Coverings|H7_Vaccination policy|H8M_Protection of elderly people|\n",
            "+----------+----------+------------------+---------------------+------------------------+------------------------------+--------------------------+-----------------------------+-------------------------------------+----------------------------------+-----------------+-----------------------+-------------------------------+-----------------+------------------+--------------------+---------------------+--------------------------------+\n",
            "|   Alabama|2020-01-01|                 0|                    0|                       0|                             0|                         0|                            0|                                    0|                                 0|                0|                      0|                              0|                0|                 0|                   0|                    0|                               0|\n",
            "|   Alabama|2020-02-01|                 0|                    0|                       0|                             0|                         0|                            0|                                    0|                                 1|                0|                      0|                              0|                1|                 1|                   0|                    0|                               0|\n",
            "|   Alabama|2020-03-01|                 0|                    0|                       0|                             0|                         0|                            0|                                    0|                                 3|                0|                      0|                              1|                1|                 1|                   0|                    0|                               0|\n",
            "|   Alabama|2020-04-01|                 3|                    3|                       2|                             4|                         1|                            2|                                    1|                                 3|                2|                      1|                              2|                1|                 1|                   0|                    0|                               3|\n",
            "|   Alabama|2020-05-01|                 3|                    3|                       2|                             4|                         1|                            2|                                    1|                                 3|                2|                      1|                              2|                2|                 1|                   3|                    0|                               3|\n",
            "|   Alabama|2020-06-01|                 3|                    3|                       2|                             0|                         1|                            2|                                    1|                                 3|                2|                      1|                              2|                2|                 1|                   3|                    0|                               3|\n",
            "|   Alabama|2020-07-01|                 3|                    1|                       1|                             0|                         1|                            1|                                    1|                                 3|                2|                      1|                              2|                3|                 1|                   3|                    0|                               3|\n",
            "|   Alabama|2020-08-01|                 3|                    1|                       1|                             0|                         1|                            1|                                    1|                                 3|                1|                      0|                              2|                3|                 1|                   3|                    0|                               3|\n",
            "|   Alabama|2020-09-01|                 2|                    2|                       1|                             0|                         0|                            1|                                    1|                                 3|                1|                      0|                              2|                3|                 2|                   3|                    0|                               3|\n",
            "|   Alabama|2020-10-01|                 2|                    1|                       1|                             0|                         0|                            1|                                    1|                                 3|                1|                      1|                              2|                3|                 1|                   3|                    0|                               3|\n",
            "+----------+----------+------------------+---------------------+------------------------+------------------------------+--------------------------+-----------------------------+-------------------------------------+----------------------------------+-----------------+-----------------------+-------------------------------+-----------------+------------------+--------------------+---------------------+--------------------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- RegionName: string (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- C1M_School closing: integer (nullable = true)\n",
            " |-- C2M_Workplace closing: integer (nullable = true)\n",
            " |-- C3M_Cancel public events: integer (nullable = true)\n",
            " |-- C4M_Restrictions on gatherings: integer (nullable = true)\n",
            " |-- C5M_Close public transport: integer (nullable = true)\n",
            " |-- C6M_Stay at home requirements: integer (nullable = true)\n",
            " |-- C7M_Restrictions on internal movement: integer (nullable = true)\n",
            " |-- C8EV_International travel controls: integer (nullable = true)\n",
            " |-- E1_Income support: integer (nullable = true)\n",
            " |-- E2_Debt/contract relief: integer (nullable = true)\n",
            " |-- H1_Public information campaigns: integer (nullable = true)\n",
            " |-- H2_Testing policy: integer (nullable = true)\n",
            " |-- H3_Contact tracing: integer (nullable = true)\n",
            " |-- H6M_Facial Coverings: integer (nullable = true)\n",
            " |-- H7_Vaccination policy: integer (nullable = true)\n",
            " |-- H8M_Protection of elderly people: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = get_data(rdd)\n",
        "data.show(10)\n",
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "jYTwc9Cgtw87"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def apriori(items, min_sup, itemset_size):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        items: list\n",
        "        min_sup: the min support\n",
        "        itemset_size : if want double (2), if want triplet (3)\n",
        "    OUTPUT:\n",
        "        list of frequent itemsets: list\n",
        "\n",
        "    NOTE: output a list of frequent itemsets.\n",
        "    \"\"\"\n",
        "    item_counts = {} # dictionary to count individual items\n",
        "\n",
        "    # frequency of each item\n",
        "    for item in items:\n",
        "        for itm in item:\n",
        "            if itm in item_counts:\n",
        "                item_counts[itm] += 1\n",
        "            else:\n",
        "                item_counts[itm] = 1\n",
        "\n",
        "    # print(\"item_counts\")\n",
        "    # print(item_counts)\n",
        "    # if itself has less count than min_support, we don't need it\n",
        "    frequent_items = {item: count for item, count in item_counts.items() if count >= min_sup}\n",
        "\n",
        "\n",
        "    frequent_itemsets = [] # list for frequent item\"sets\"\n",
        "\n",
        "    # create Double\n",
        "    if itemset_size == 2:\n",
        "        candidates = []\n",
        "\n",
        "        # group to make candidate sets\n",
        "        for item_combination in combinations(frequent_items.keys(), itemset_size):\n",
        "            candidate_itemset = tuple(sorted(item_combination))\n",
        "            candidates.append(candidate_itemset)\n",
        "\n",
        "        # frequency for candidate sets\n",
        "        candidate_counts = {}\n",
        "        for transaction in transactions:\n",
        "            for itemset in candidates:\n",
        "                if set(itemset).issubset(set(transaction)):\n",
        "                    if itemset in candidate_counts:\n",
        "                        candidate_counts[itemset] += 1\n",
        "                    else:\n",
        "                        candidate_counts[itemset] = 1\n",
        "\n",
        "        # only keep the sets with their freq >= min_support\n",
        "        frequent_itemsets.extend([itemset for itemset, count in candidate_counts.items() if count >= min_sup])\n",
        "\n",
        "\n",
        "    # create Triplet\n",
        "    elif itemset_size == 3:\n",
        "        candidates = []\n",
        "\n",
        "        # group to make candidate sets\n",
        "        for item_combination in combinations(frequent_items.keys(), itemset_size):\n",
        "            candidate_itemset = tuple(sorted(item_combination))\n",
        "            candidates.append(candidate_itemset)\n",
        "\n",
        "        # frequency for candidate sets\n",
        "        candidate_counts = {}\n",
        "        for transaction in transactions:\n",
        "            for itemset in candidates:\n",
        "                if set(itemset).issubset(set(transaction)):\n",
        "                    if itemset in candidate_counts:\n",
        "                        candidate_counts[itemset] += 1\n",
        "                    else:\n",
        "                        candidate_counts[itemset] = 1\n",
        "\n",
        "        # only keep the sets with their freq >= min_support\n",
        "        frequent_itemsets.extend([itemset for itemset, count in candidate_counts.items() if count >= min_sup])\n",
        "\n",
        "    # with 'if' statements, double and triplet are seperated so the output only contains double / triplet depending on the input.\n",
        "\n",
        "    return frequent_itemsets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE\n",
        "min_support = 100\n",
        "itemset_size = 2\n",
        "\n",
        "columns = data.columns\n",
        "transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] != 0]).collect()\n",
        "\n",
        "# for tran in transactions:\n",
        "#   print(tran)\n",
        "\n",
        "frequent_doubles = apriori(transactions, min_support, itemset_size)\n",
        "\n",
        "for double in frequent_doubles:\n",
        "    print(double)\n",
        "\n",
        "\n",
        "# =============== #\n",
        "\n",
        "\n",
        "# # TRIPLET\n",
        "# min_support = 80\n",
        "# itemset_size = 3\n",
        "\n",
        "# columns = data.columns\n",
        "# transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] > 0]).collect()\n",
        "\n",
        "# frequent_triplets = apriori(transactions, min_support, itemset_size)\n",
        "\n",
        "# for triplet in frequent_triplets:\n",
        "#     print(triplet)"
      ],
      "metadata": {
        "id": "z9pMpSWUinaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e985eab-2589-4daf-f4b8-1abde0143f79"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('C8EV_International travel controls', 'H2_Testing policy')\n",
            "('C8EV_International travel controls', 'H3_Contact tracing')\n",
            "('H2_Testing policy', 'H3_Contact tracing')\n",
            "('C8EV_International travel controls', 'H1_Public information campaigns')\n",
            "('H1_Public information campaigns', 'H2_Testing policy')\n",
            "('H1_Public information campaigns', 'H3_Contact tracing')\n",
            "('C1M_School closing', 'C8EV_International travel controls')\n",
            "('C2M_Workplace closing', 'C8EV_International travel controls')\n",
            "('C3M_Cancel public events', 'C8EV_International travel controls')\n",
            "('C4M_Restrictions on gatherings', 'C8EV_International travel controls')\n",
            "('C5M_Close public transport', 'C8EV_International travel controls')\n",
            "('C6M_Stay at home requirements', 'C8EV_International travel controls')\n",
            "('C7M_Restrictions on internal movement', 'C8EV_International travel controls')\n",
            "('C8EV_International travel controls', 'E1_Income support')\n",
            "('C8EV_International travel controls', 'E2_Debt/contract relief')\n",
            "('C8EV_International travel controls', 'H8M_Protection of elderly people')\n",
            "('C1M_School closing', 'H2_Testing policy')\n",
            "('C2M_Workplace closing', 'H2_Testing policy')\n",
            "('C3M_Cancel public events', 'H2_Testing policy')\n",
            "('C4M_Restrictions on gatherings', 'H2_Testing policy')\n",
            "('C5M_Close public transport', 'H2_Testing policy')\n",
            "('C6M_Stay at home requirements', 'H2_Testing policy')\n",
            "('C7M_Restrictions on internal movement', 'H2_Testing policy')\n",
            "('E1_Income support', 'H2_Testing policy')\n",
            "('E2_Debt/contract relief', 'H2_Testing policy')\n",
            "('H2_Testing policy', 'H8M_Protection of elderly people')\n",
            "('C1M_School closing', 'H3_Contact tracing')\n",
            "('C2M_Workplace closing', 'H3_Contact tracing')\n",
            "('C3M_Cancel public events', 'H3_Contact tracing')\n",
            "('C4M_Restrictions on gatherings', 'H3_Contact tracing')\n",
            "('C5M_Close public transport', 'H3_Contact tracing')\n",
            "('C6M_Stay at home requirements', 'H3_Contact tracing')\n",
            "('C7M_Restrictions on internal movement', 'H3_Contact tracing')\n",
            "('E1_Income support', 'H3_Contact tracing')\n",
            "('E2_Debt/contract relief', 'H3_Contact tracing')\n",
            "('H3_Contact tracing', 'H8M_Protection of elderly people')\n",
            "('C1M_School closing', 'H1_Public information campaigns')\n",
            "('C2M_Workplace closing', 'H1_Public information campaigns')\n",
            "('C3M_Cancel public events', 'H1_Public information campaigns')\n",
            "('C4M_Restrictions on gatherings', 'H1_Public information campaigns')\n",
            "('C5M_Close public transport', 'H1_Public information campaigns')\n",
            "('C6M_Stay at home requirements', 'H1_Public information campaigns')\n",
            "('C7M_Restrictions on internal movement', 'H1_Public information campaigns')\n",
            "('E1_Income support', 'H1_Public information campaigns')\n",
            "('E2_Debt/contract relief', 'H1_Public information campaigns')\n",
            "('H1_Public information campaigns', 'H8M_Protection of elderly people')\n",
            "('C1M_School closing', 'C2M_Workplace closing')\n",
            "('C1M_School closing', 'C3M_Cancel public events')\n",
            "('C1M_School closing', 'C4M_Restrictions on gatherings')\n",
            "('C1M_School closing', 'C5M_Close public transport')\n",
            "('C1M_School closing', 'C6M_Stay at home requirements')\n",
            "('C1M_School closing', 'C7M_Restrictions on internal movement')\n",
            "('C1M_School closing', 'E1_Income support')\n",
            "('C1M_School closing', 'E2_Debt/contract relief')\n",
            "('C1M_School closing', 'H8M_Protection of elderly people')\n",
            "('C2M_Workplace closing', 'C3M_Cancel public events')\n",
            "('C2M_Workplace closing', 'C4M_Restrictions on gatherings')\n",
            "('C2M_Workplace closing', 'C5M_Close public transport')\n",
            "('C2M_Workplace closing', 'C6M_Stay at home requirements')\n",
            "('C2M_Workplace closing', 'C7M_Restrictions on internal movement')\n",
            "('C2M_Workplace closing', 'E1_Income support')\n",
            "('C2M_Workplace closing', 'E2_Debt/contract relief')\n",
            "('C2M_Workplace closing', 'H8M_Protection of elderly people')\n",
            "('C3M_Cancel public events', 'C4M_Restrictions on gatherings')\n",
            "('C3M_Cancel public events', 'C5M_Close public transport')\n",
            "('C3M_Cancel public events', 'C6M_Stay at home requirements')\n",
            "('C3M_Cancel public events', 'C7M_Restrictions on internal movement')\n",
            "('C3M_Cancel public events', 'E1_Income support')\n",
            "('C3M_Cancel public events', 'E2_Debt/contract relief')\n",
            "('C3M_Cancel public events', 'H8M_Protection of elderly people')\n",
            "('C4M_Restrictions on gatherings', 'C5M_Close public transport')\n",
            "('C4M_Restrictions on gatherings', 'C6M_Stay at home requirements')\n",
            "('C4M_Restrictions on gatherings', 'C7M_Restrictions on internal movement')\n",
            "('C4M_Restrictions on gatherings', 'E1_Income support')\n",
            "('C4M_Restrictions on gatherings', 'E2_Debt/contract relief')\n",
            "('C4M_Restrictions on gatherings', 'H8M_Protection of elderly people')\n",
            "('C5M_Close public transport', 'C6M_Stay at home requirements')\n",
            "('C5M_Close public transport', 'C7M_Restrictions on internal movement')\n",
            "('C5M_Close public transport', 'E1_Income support')\n",
            "('C5M_Close public transport', 'E2_Debt/contract relief')\n",
            "('C5M_Close public transport', 'H8M_Protection of elderly people')\n",
            "('C6M_Stay at home requirements', 'C7M_Restrictions on internal movement')\n",
            "('C6M_Stay at home requirements', 'E1_Income support')\n",
            "('C6M_Stay at home requirements', 'E2_Debt/contract relief')\n",
            "('C6M_Stay at home requirements', 'H8M_Protection of elderly people')\n",
            "('C7M_Restrictions on internal movement', 'E1_Income support')\n",
            "('C7M_Restrictions on internal movement', 'E2_Debt/contract relief')\n",
            "('C7M_Restrictions on internal movement', 'H8M_Protection of elderly people')\n",
            "('E1_Income support', 'E2_Debt/contract relief')\n",
            "('E1_Income support', 'H8M_Protection of elderly people')\n",
            "('E2_Debt/contract relief', 'H8M_Protection of elderly people')\n",
            "('C8EV_International travel controls', 'H6M_Facial Coverings')\n",
            "('H2_Testing policy', 'H6M_Facial Coverings')\n",
            "('H3_Contact tracing', 'H6M_Facial Coverings')\n",
            "('H1_Public information campaigns', 'H6M_Facial Coverings')\n",
            "('C1M_School closing', 'H6M_Facial Coverings')\n",
            "('C2M_Workplace closing', 'H6M_Facial Coverings')\n",
            "('C3M_Cancel public events', 'H6M_Facial Coverings')\n",
            "('C4M_Restrictions on gatherings', 'H6M_Facial Coverings')\n",
            "('C5M_Close public transport', 'H6M_Facial Coverings')\n",
            "('C6M_Stay at home requirements', 'H6M_Facial Coverings')\n",
            "('C7M_Restrictions on internal movement', 'H6M_Facial Coverings')\n",
            "('E1_Income support', 'H6M_Facial Coverings')\n",
            "('E2_Debt/contract relief', 'H6M_Facial Coverings')\n",
            "('H6M_Facial Coverings', 'H8M_Protection of elderly people')\n",
            "('C8EV_International travel controls', 'H7_Vaccination policy')\n",
            "('H2_Testing policy', 'H7_Vaccination policy')\n",
            "('H3_Contact tracing', 'H7_Vaccination policy')\n",
            "('H1_Public information campaigns', 'H7_Vaccination policy')\n",
            "('C1M_School closing', 'H7_Vaccination policy')\n",
            "('C2M_Workplace closing', 'H7_Vaccination policy')\n",
            "('C3M_Cancel public events', 'H7_Vaccination policy')\n",
            "('C6M_Stay at home requirements', 'H7_Vaccination policy')\n",
            "('C7M_Restrictions on internal movement', 'H7_Vaccination policy')\n",
            "('E1_Income support', 'H7_Vaccination policy')\n",
            "('E2_Debt/contract relief', 'H7_Vaccination policy')\n",
            "('H7_Vaccination policy', 'H8M_Protection of elderly people')\n",
            "('H6M_Facial Coverings', 'H7_Vaccination policy')\n",
            "('C4M_Restrictions on gatherings', 'H7_Vaccination policy')\n",
            "('C5M_Close public transport', 'H7_Vaccination policy')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCZOK6SIxfXg"
      },
      "source": [
        "Run your function in the next cells to output required content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg9Xc5YvFoxH"
      },
      "source": [
        "3C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3xvcXw4Itbx",
        "outputId": "04f5337a-fb11-468e-ee9d-88f10edf8c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============== DOUBLE ==============\n",
            "C1M_School closing => C8EV_International travel controls, Confidence: 1.0000\n",
            "C2M_Workplace closing => C8EV_International travel controls, Confidence: 1.0000\n",
            "C3M_Cancel public events => C8EV_International travel controls, Confidence: 1.0000\n",
            "C4M_Restrictions on gatherings => C8EV_International travel controls, Confidence: 1.0000\n",
            "C8EV_International travel controls => H2_Testing policy, Confidence: 1.0000\n",
            "C8EV_International travel controls => H3_Contact tracing, Confidence: 1.0000\n",
            "H1_Public information campaigns => C8EV_International travel controls, Confidence: 1.0000\n",
            "H1_Public information campaigns => H2_Testing policy, Confidence: 1.0000\n",
            "H1_Public information campaigns => H3_Contact tracing, Confidence: 1.0000\n",
            "H2_Testing policy => C8EV_International travel controls, Confidence: 1.0000\n",
            "============== TRIPLE ==============\n",
            "('C1M_School closing', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C1M_School closing', 'C8EV_International travel controls') => H3_Contact tracing, Confidence: 1.000\n",
            "('C1M_School closing', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C1M_School closing', 'H3_Contact tracing') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C2M_Workplace closing', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C2M_Workplace closing', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C3M_Cancel public events', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C3M_Cancel public events', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C4M_Restrictions on gatherings', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C4M_Restrictions on gatherings', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C5M_Close public transport', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C5M_Close public transport', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C6M_Stay at home requirements', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n",
            "('C6M_Stay at home requirements', 'H2_Testing policy') => C8EV_International travel controls, Confidence: 1.000\n",
            "('C7M_Restrictions on internal movement', 'C8EV_International travel controls') => H2_Testing policy, Confidence: 1.000\n"
          ]
        }
      ],
      "source": [
        "def calculate_freq(group, transactions, double_triplet):\n",
        "  if double_triplet == 2: # computation for double\n",
        "    X, Y = group\n",
        "    count_XY = 0\n",
        "\n",
        "    for transaction in transactions:\n",
        "        if X in transaction and Y in transaction:\n",
        "            count_XY += 1\n",
        "    return count_XY\n",
        "\n",
        "  elif double_triplet == 3: # computation for triplet\n",
        "    X, Y, Z = group\n",
        "    count_XYZ = 0\n",
        "\n",
        "    for transaction in transactions:\n",
        "        if X in transaction and Y in transaction and Z in transaction:\n",
        "            count_XYZ += 1\n",
        "    return count_XYZ\n",
        "\n",
        "def calculate_confidence(A, B, transactions):\n",
        "  # conf(A -> B) =  P(B|A)\n",
        "  # P(B|A) = P(AB) / P(A)\n",
        "  #        = (num of transc. containing both / all transc. ) * (all transc. / num of transc. containing A)\n",
        "  #        = num of transc. containing both / num of transc. containing A\n",
        "  count_A = 0\n",
        "  count_A_B = 0\n",
        "\n",
        "  for transaction in transactions:\n",
        "    if set(A).issubset(set(transaction)):\n",
        "        count_A += 1\n",
        "        if B in transaction:\n",
        "            count_A_B += 1\n",
        "\n",
        "  #print(\"A:\", A, \"B:\", B, count_A_B / count_A)\n",
        "  return count_A_B / count_A if count_A > 0 else 0\n",
        "\n",
        "\n",
        "def generate_association_rules(double_triple, frequent_group, transactions, min_support, return_count):\n",
        "    association_rules = []\n",
        "\n",
        "    if double_triple == 2: # for double\n",
        "      for pair in frequent_group[:return_count]:\n",
        "        X, Y = pair\n",
        "\n",
        "        count_XY = calculate_freq(pair, transactions, double_triple)\n",
        "\n",
        "        if count_XY >= min_support:\n",
        "            confidence_XY = calculate_confidence([X], Y, transactions)\n",
        "            confidence_YX = calculate_confidence([Y], X, transactions)\n",
        "\n",
        "            association_rules.append((X, Y, confidence_XY))\n",
        "            association_rules.append((Y, X, confidence_YX))\n",
        "\n",
        "\n",
        "    elif double_triple == 3: # for triplet\n",
        "      for triplet in frequent_group[:return_count]:\n",
        "          X, Y, Z = triplet\n",
        "\n",
        "          count_XYZ = calculate_freq(triplet, transactions, double_triple)\n",
        "\n",
        "          if count_XYZ >= min_support:\n",
        "              confidence_XY_Z = calculate_confidence([X, Y], Z, transactions)\n",
        "              confidence_X_Z_Y = calculate_confidence([X, Z], Y, transactions)\n",
        "              confidence_Y_Z_X = calculate_confidence([Y, Z], X, transactions)\n",
        "\n",
        "              association_rules.append(((X, Y), Z, confidence_XY_Z))\n",
        "              association_rules.append(((X, Z), Y, confidence_X_Z_Y))\n",
        "              association_rules.append(((Y, Z), X, confidence_Y_Z_X))\n",
        "\n",
        "    # sort the rules in order we want\n",
        "    association_rules.sort(key= lambda rule: (-rule[2], rule[0], rule[1])) # rule[2] = confidence, add - to sort in descending order, then alphabetically (lexologically) break tie for the same conf level\n",
        "\n",
        "    return association_rules[:return_count]\n",
        "\n",
        "# Run the function - Double\n",
        "print(\"============== DOUBLE ==============\")\n",
        "min_support = 100\n",
        "itemset_size = 2\n",
        "\n",
        "columns = data.columns\n",
        "transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] > 0]).collect()\n",
        "frequent_group = apriori(transactions, min_support, itemset_size)\n",
        "res = generate_association_rules(itemset_size, frequent_group, transactions, min_support, return_count=10)\n",
        "\n",
        "for rule in res:\n",
        "    A, B, confidence = rule\n",
        "    print(f\"{A} => {B}, Confidence: {confidence:.4f}\")\n",
        "\n",
        "\n",
        "# Run the function - Triple\n",
        "print(\"============== TRIPLE ==============\")\n",
        "min_support = 100\n",
        "itemset_size = 3\n",
        "\n",
        "columns = data.columns\n",
        "transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] > 0]).collect()\n",
        "frequent_group = apriori(transactions, min_support, itemset_size)\n",
        "res = generate_association_rules(itemset_size, frequent_group, transactions, min_support, return_count=15)\n",
        "\n",
        "for rule in res:\n",
        "    A, B, confidence = rule\n",
        "    print(f\"{A} => {B}, Confidence: {confidence:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "9zcpdpViN32Q"
      },
      "outputs": [],
      "source": [
        "# OVERALL - SAVE AS TXT FILE\n",
        "\n",
        "\n",
        "# Save Double\n",
        "min_support = 100\n",
        "itemset_size = 2\n",
        "\n",
        "columns = data.columns\n",
        "transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] > 0]).collect()\n",
        "frequent_group = apriori(transactions, min_support, itemset_size)\n",
        "res = generate_association_rules(itemset_size, frequent_group, transactions, min_support, return_count=10)\n",
        "\n",
        "with open('Q3_b.txt', 'w') as saveFile:\n",
        "  for rule in res:\n",
        "    A, B, confidence = rule\n",
        "    output = f\"{A},{B},{confidence:.3f}\"\n",
        "    saveFile.write(str(output))\n",
        "    saveFile.write(\"\\n\")\n",
        "\n",
        "\n",
        "# Save Triple\n",
        "\n",
        "min_support = 100\n",
        "itemset_size = 3\n",
        "\n",
        "columns = data.columns\n",
        "transactions = data.rdd.map(lambda row: [column for column in columns if isinstance(row[column], int) and row[column] > 0]).collect()\n",
        "frequent_group = apriori(transactions, min_support, itemset_size)\n",
        "res = generate_association_rules(itemset_size, frequent_group, transactions, min_support, return_count=15)\n",
        "\n",
        "with open('Q3_c.txt', 'w') as saveFile:\n",
        "  for rule in res:\n",
        "    A, B, confidence = rule\n",
        "    output = f\"{A},{B},{confidence:.3f}\"\n",
        "    saveFile.write(str(output))\n",
        "    saveFile.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3tBl3GdZ_fn"
      },
      "execution_count": 114,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}